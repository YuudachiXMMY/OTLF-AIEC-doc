# AIEC Contest Platform — Product Requirements (PRD)

**Version:** 1.0
**Date:** 2026-01-20
**Status:** Draft for implementation (3-week MVP launch)

**Introduction:** This document outlines the functional requirements, user stories, and high-level workflows for the **High School AI Entrepreneurship Contest (AIEC) Platform**. The platform is designed for the annual AIEC competition organized by OTLF, supporting end-to-end contest operations (registration, team formation, submissions, judging, results). It emphasizes fairness and transparency in line with contest rules – the contest includes multi-stage evaluations (Preliminary, Semi-Final, Final) and strict fairness mechanisms like blind reviews and conflict-of-interest avoidance. This PRD is intended for the development team and stakeholders to understand what the system must do to successfully run the contest.

## 0. Scope and Goal

Build a full replacement contest management platform for the High School AIEC, supporting:
- **Public site & Secure Login:** A marketing site (public portal) and a secure, role-based portal for authenticated users (students, teachers, judges, admins).
- **Team Formation:** Team creation by students, join via code, and mentor (teacher) binding requirement.
- **Multi-Contest, Multi-Edition:** Ability to host multiple contests and yearly editions. Each edition can have its own stages and configuration.
- **Multi-Stage Submissions:** Configurable required materials per stage (e.g. business plan, prototype, etc.), aligning with the contest’s Preliminary/Semi-Final/Final submission requirements.
- **Blind Judging & Fairness:** Blind review of submissions (teams are anonymized for judges), plagiarism/compliance pre-screening, automated judge assignment, rubric-based scoring, and enforcement of contest fairness rules (e.g. conflict-of-interest opt-out, outlier score exclusion).
- **Stage Results & Awards:** Computation of stage results (which teams advance or awards received) and final awards.
- **Admin Functions:** Administrative console for contest content management (pages, news), user management, approving applications, assigning judges, aggregating scores, publishing results, and audit logs for transparency.

**Out of Scope (MVP):**
- Payment processing (if contest fees are required, assume manual or stub for now).
- Real-time collaborative editing of submissions.
- Advanced conflict-of-interest (COI) graph analysis beyond explicit declarations by judges (basic COI handling is in scope).

## 1. Personas and Roles

- **Student (Participant):** High school student competitor. Can register, join/create a team, submit project materials, and view results/feedback.
- **Teacher/Mentor (Team Supervisor):** Typically a teacher or adult mentor for the team. Registers in the system to be linked to a team as the required mentor. Can help view team submissions and possibly submit on behalf of team if needed.
- **Judge (Reviewer):** Contest judge who scores submissions. Judges have access only to assigned teams’ anonymized submissions, scoring forms, and must provide feedback.
- **Admin (Organizer):** Contest administrator (OTLF staff or organizer). Has full access to manage contests, approve teams, configure stages, assignments, view all submissions and scores, publish results, etc. Admin accounts require higher security (MFA).
## 2. Product Principles

- **Fairness-by-Design:** The platform enforces blind judging, COI avoidance, auditable scoring, and publishing of feedback to ensure the contest is fair and impartial. No judge sees team identities during evaluation, and scoring algorithms automatically drop outlier scores for fairness (see Section 5.6).
- **Operational Clarity:** Each contest edition is isolated with explicit stages, requirements, and deadlines. Users see only relevant actions (e.g. students only see their team and submission status, judges only see assigned tasks). The workflow for each phase is clearly delineated.
- **Configurable (Data-Driven):** Avoid hard-coding contest specifics. Rubrics, submission requirements, timelines, and advancement logic are data-driven per edition/stage. This allows contest rules changes (e.g. scoring weights or new stages) without code changes.
## 3. System Decomposition (Sub-Systems)

### 3.1 Portal (Public Site)

**Purpose:** Public-facing website for general information about the contest. It includes pages for contest overview, timelines, announcements, FAQs, and provides entry points for user registration/login. This is the marketing and informational front-end for the contest.

### 3.2 Participant Portal (Team Submission)

**Purpose:** Secure area for students and teachers. Here, a student (or teacher on behalf of a team) can create a new team or join an existing team via an invite code. Teams must bind a mentor teacher before entering the contest. The team portal allows filling out the initial application (project abstract, team roles), uploading required submission files for each stage, and viewing feedback and results.

### 3.3 Judge Portal (Review Interface)

**Purpose:** Secure interface for judges to review and score submissions assigned to them. Judges see a list of their assigned projects (with teams anonymized by code). For each assignment, judges can download/view submission materials, fill in a rubric-based score form for each criterion, and provide the required minimum length written feedback. Judges can also declare a conflict of interest and decline an assignment with a reason, which notifies admins to reassign.

### 3.4 Admin Console

**Purpose:** Back-office interface for contest administrators. Admins can create contests and editions, define stage configurations (deadlines, required submission items, rubrics), manage user accounts and roles, approve or reject team applications, and manage judge assignments. The admin console also provides features to monitor judging progress (who has submitted scores), run score aggregation (apply outlier removal and calculate results), publish results or news updates, and view audit logs of critical actions.

## 4. End-to-End Workflow (MVP)

- **Registration:** Students and teachers register accounts via the portal and then log in. (Judges and admins may be pre-created or invited by admins.)
- **Team Formation:** A student designated as team leader creates a new team under a specific contest edition. The system generates a unique team join code. The team leader shares this code with other student members. Students join the team using the code. The team must also bind a mentor (teacher account) to the team – the mentor can join via the code or be linked by the team leader/admin.
- **Application Submission:** The team (once formed and with a mentor attached) fills out an initial application to officially enter the contest. This includes a project abstract (up to 500 words) and each member’s role statement. They must also acknowledge a mentor’s oversight and originality/integrity agreements. The application cannot be submitted until all required fields are filled and the team meets composition rules (team size within 5–8, and covering required roles: at least one technical, one business, one social responsibility role). The team submits the application.
- **Application Approval:** An Admin reviews the submitted team application. The admin verifies that the project proposal meets basic requirements and that the team has the required mentor and composition. The admin then approves the application (or rejects it with feedback if it doesn’t meet criteria). Once approved, the team’s status becomes **ACTIVE** for the contest. (Only active teams advance to contest stages.)
- **Stage Submissions:** As the contest progresses, each stage (e.g. Preliminary round, Semi-finals, Final) opens according to the schedule. For an open stage, active teams can upload the required submission materials (e.g. documents, videos, links) via the platform. The system validates that all required items for the stage are provided (for example, in the Preliminary round, perhaps a business plan PDF and a demo video link are required; in the Final, additional materials like pitch deck, technical report, etc., might be required). Teams can save a draft and then finalize **Submit** before the deadline. After the stage deadline (plus any grace period), the submission is locked – no further edits or late submissions (unless admin manually grants an exception).
- **Anonymization & Judge Assignment:** Once submissions are in and the stage closes, the system generates an **anonymous code** for each team’s submission package (if not already generated). Admin triggers the assignment of judges to submissions for that stage. The system assigns each submission to a set number of judges (e.g. 3 judges per project) in a balanced manner (ensuring roughly equal load per judge). Judges cannot see any team-identifying info – they only see the anonymous project code and materials. If a judge identifies a conflict of interest with an assigned project (e.g. they recognize the work or participants), they can decline the assignment with a stated reason, and the admin can reassign that project to another judge.
- **Judging & Scoring:** Judges log in to their portal to see **My Assignments** for the stage. For each assignment, the judge downloads the submission materials (via secure links) and reviews them. The judge fills out the score sheet: scoring each rubric criterion (numerical scores which will be weighted) and writing an overall comment. The system enforces business rules here: all required criteria must be scored, and the written feedback must meet a minimum word count (e.g. ≥60 words for preliminary, ≥80 words for semi-final feedback). Judges submit their scores which are stored as draft; they can edit their scores until the scoring deadline or until they mark it final. After the judging period, any missing scores are flagged for admins to follow up.
- **Aggregation of Results:** When all (or enough) judges have submitted their evaluations, the admin triggers score aggregation for the stage. The system calculates each team’s total score by computing the weighted average of all judge scores, and applies an **outlier removal** rule: if a team had a large judging panel (e.g. 5 or more judges), the highest and lowest total score for that team are dropped to eliminate outliers. The final score is then the average of the remaining scores. The system produces a ranking of teams for the stage. For advancement stages (Preliminary -> Semi, Semi -> Final), the admin uses these rankings (and possibly additional criteria or quotas) to decide which teams advance. The platform can automatically flag the top X teams or those above a cutoff for advancement. (If scores are very close around the cutoff, the rules allow the committee to discuss borderline cases ±0.2 points if needed, though this may happen outside the system.)
- **Publishing Results:** The admin reviews the aggregated results and then publishes the stage results. When results are published, teams can log in to see their outcome (e.g. whether they advanced or their award in that stage) and access the feedback. The system compiles an anonymized feedback report (“feedback pack”) for each team, aggregating all judges’ comments and scores. Team members (and their mentor) can download or view this feedback pack. For public transparency, the admin may also publish key results on the public site (such as a list of finalists or winners, without revealing judge identities or detailed scores).
- **Finals and Awards:** In the Final round, the process includes live presentations which may involve judges entering scores in real-time (the system supports input of final scores and perhaps printing score sheets). After final aggregation, the admin records the winners (champion, 2nd place, etc., and any special awards). The system can store these awards per team. Finally, the admin publishes the winners on the public site and closes out the contest edition. An optional feedback pack is provided to finalists with judges’ comments from the final round as well.

Throughout these steps, the platform logs key actions (submissions, score changes, approvals, publications) in an **audit log** for accountability. All user notifications (e.g. team approved, assignment emails to judges, results published) are handled via email or in-app notifications (MVP can use email or simple on-screen alerts).

## 5. MVP Functional Requirements (Priority 0)

Below are the core functional requirements for the MVP, grouped by feature area. Each requirement is critical for the initial launch.

### 5.1 Authentication & RBAC

- **User Registration & Login:** Support user sign-up for Students and Teachers (mentors) with email and password. Judges and Admin accounts will be created by Admins (or seeded) rather than open registration. Standard email + password login for all.
- **JWT-Based Sessions:** Upon successful login, the backend issues a JWT for session management. The front-end stores this token (e.g. in memory or localStorage) and includes it in Authorization: Bearer headers for subsequent requests.
- **Role-Based Access Control:** Implement role checks on both frontend routing and backend endpoints. Frontend should route users to different dashboards based on role, and guard pages (e.g. a judge cannot access admin pages). Backend APIs should verify the JWT and the user’s role for each protected route.
- **Admin MFA:** Admin users require multi-factor authentication for login. MVP approach: allow admin to enable TOTP (time-based one-time password) using an authenticator app. Provide endpoints to set up (/auth/mfa/enable) and verify code (/auth/mfa/verify). Alternatively (or additionally), an email OTP can be used. The goal is extra security for admin accounts due to their elevated privileges.
### 5.2 Contest Configuration (Contest, Edition, Stage)

- **Contest & Edition Management:** The system supports multiple contests (for extensibility, though initially there is one AIEC contest). Each contest can have multiple editions (years). Key properties: contest name, edition year, edition title, registration period, etc. Admins can create a contest (if needed) and annual editions.
- **Stage Configuration:** Each edition consists of one or more stages (e.g. Registration/Application, Preliminary, Semi-Final, Final). For each stage, the admin can configure: a name, type (registration, prelim, etc.), start date, submission deadline, grace period (extra minutes after deadline where submissions are allowed, if any), and lock date/time (after which no submissions or scoring changes can occur).
- **Stage Requirements:** Admin can define what submission materials are required for each stage. For each material type (e.g. Business Plan document, Demo Video, Pitch Deck, etc.), the configuration includes: content type (file upload, text input, or link), whether it’s required or optional, file type/size limits, and any text length limits. These requirements tie to the frontend form validation for submissions. (For MVP, a set of common material types is predefined in the system, see Data Dictionary under material_types for examples.)
- **Rubric Configuration:** Each stage is associated with a scoring rubric. Admin can create a rubric template (set of criteria) with each criterion having a description and weight. Also define the scoring scale (e.g. 1–10) and the minimum comment length for that stage’s feedback (e.g. 60 words for prelim, 80 for semi, etc., as per contest rules). Judges will use this rubric when scoring.
### 5.3 Teams & Applications

- **Team Creation:** A student (or teacher) can create a new team for a given contest edition. Creating a team generates a unique alphanumeric **join code** that others can use to join. The creator becomes the team leader by default. Team properties include a team name (must be unique within the edition) and an optional short introduction/description.
- **Team Join via Code:** Users can join an existing team by entering the team’s join code. The system validates the code and adds the user as a team member if it’s valid. There are constraints: team size cannot exceed the maximum (8 members), and users cannot join multiple teams in the same contest edition (one team per participant).
- **Team Management:** Team members (especially the leader) can view the current team roster (members’ names and roles) and see the mentor assignment status. The team leader can remove members if needed (except they cannot remove the required mentor or leave the team without assigning a new leader). A mechanism to change team leader or remove a student might be included (MVP can allow leader to remove others, and admin can intervene if needed).
- **Mentor Binding:** Every team **must** have a mentor (teacher role) assigned before submitting the application. The mentor is typically a teacher account that either the team invites (by sharing the join code with their teacher) or an existing teacher user that the team selects (if the teacher registered separately). MVP approach: treat mentors like team members with the Teacher role – once a Teacher joins via the code, the system marks them as the team’s mentor. (Alternatively, an admin can assign a mentor to a team if needed.) Only one mentor per team is supported in MVP.
- **Participation Application:** Once the team is formed and has a mentor, the team must submit an “application” to enter the contest (this corresponds to the Registration stage). The application includes:
- **Project Abstract:** A text field (up to 500 words) where the team outlines their project idea (covering problem, AI solution, business concept). This ensures teams have done some preliminary thinking and not just a blank idea.
- **Team Role Statements:** Each member provides a brief statement of their anticipated role/responsibility on the team (e.g. “Alice – handling AI model development”, “Bob – business plan and market research”). These statements are around 50–500 characters each. Additionally, each member selects their role category/tag: Tech, Business, Social/Impact, Design, Operations, and one member as Team Leader. These tags help ensure the team covers the required competency areas.
- **Acknowledgements:** The team must check acknowledgements that (a) the mentor has reviewed and approved their application (mentor declaration), and (b) all work is original and they agree to contest integrity rules. These are booleans that must be true to submit.
- **Application Submission Rules:** The platform validates that all required fields are provided. Team composition rules are also checked: team size is within the allowed range (5–8), a mentor is bound, and the roles coverage is adequate (at least one member tagged in each of the required categories: Technical, Business, Social, plus a designated leader). Additionally, each role statement must meet minimum length (e.g. >=50 chars) to ensure thoughtfulness. Only when all conditions are met can the team **Submit Application**. After submission, the application record is created with status “SUBMITTED” and timestamp. The team can no longer edit the application unless an admin rejects it (which would allow resubmission).
### 5.4 Submissions (Stage Deliverables)

- **Stage Submission Instances:** For each stage beyond the initial application, a team will create a **submission**. The system allows multiple versions of a submission up until the deadline. Each time a team “starts a submission” for a stage, a new version (v1, v2, ...) record is created. This versioning lets teams submit early, get feedback (if any), and resubmit updated versions (if the contest rules allow multiple attempts). MVP can support at least saving a draft and one final submission; versioning for multiple submissions is nice-to-have.
- **Submission Content:** A submission consists of one or more items, as defined by the stage’s requirements. Common item types include: file uploads (e.g. PDF documents, images), links (URLs to e.g. demo videos), or text entries. Each item will be associated with a predefined **material type code** (e.g. BP = Business Plan, MVP_VIDEO = prototype video link, PITCH_DECK, AI_NOTE, etc.). The system must enforce the rules for each item: for files, check file extension and size; for text, check word count limits; for links, maybe validate URL format. For example, in the Preliminary round, required items might be a Business Plan PDF and a Prototype Video link, whereas in Finals, additional items like a Pitch Deck and an Ethics Statement text might be required.
- **Ethics Statement (Finals):** The contest places importance on ethical AI considerations. In the Final stage, teams are required to submit an **AI Ethics Statement** (a short essay, e.g. ~150–200 words) explaining how they addressed ethics, fairness, or sustainability in their project. This is treated as a required text item in the final stage submission (and could be optional or not present in earlier stages).
- **Draft vs Submitted:** When a team is working on a submission, it remains in DRAFT status. Team members can upload files, save progress, etc. Once they are satisfied, they click **Submit** for that stage. Submission is then marked SUBMITTED with a timestamp. After the deadline passes (plus grace period), the system automatically locks all submissions (status = LOCKED) to prevent any modifications. If a team fails to submit by deadline, their latest draft might be automatically marked as final (or considered missing – MVP decision: could allow admin to mark late submissions with a status).
- **File Storage:** All uploaded files are stored in an object storage service (MinIO in development, S3 in production). The system does not store files in the database, only metadata and a reference (key or URL) to the stored file. Each file upload goes through a two-step process: (1) frontend requests a **presigned URL** for upload, (2) the file is PUT to storage, then (3) the frontend notifies the backend to mark the file as uploaded and attach it to a submission item. This offloads large file handling to the storage service.
### 5.5 Blind Review & Judge Assignment

- **Team Anonymization:** To ensure blind judging, the system will generate a random **anonymous code** for each team’s submission per stage. This code (e.g. "P17-03") is used to identify the project to judges instead of the team name or school. The mapping of team <-> anonymous code is stored securely (only admins can see it). New codes are generated each stage to prevent judges from tracking a team across rounds (if needed).
- **Assignment Generation:** Admin can initiate the assignment of judges to submissions for a stage. The admin specifies how many judges per project (e.g. 3) and an optional max assignments per judge to balance load. The system then goes through the pool of judges assigned to that contest edition and assigns each submission to judges, taking into account load balancing. Assignment logic might be simple round-robin or random distribution in MVP. Each assignment record pairs a judge, a stage, and an anonymous project code. Initially, assignment status = ASSIGNED.
- **COI Handling:** Judges should have a way to opt-out of an assignment if they recognize the project or have any conflict. In the judge interface, for each assignment there is an option **“Decline due to Conflict”** where the judge must select a reason (or type a reason) for declining. When a judge declines, the assignment status becomes DECLINED and the admin is notified. The admin can then manually assign a different judge to that project (the system could also suggest an available judge to reassign).
### 5.6 Scoring & Feedback (Rubric Enforcement)

- **Rubric-Based Scoring:** For each stage, judges fill out a score form defined by that stage’s rubric. The form lists each criterion (with a short description) and allows the judge to enter a numeric score for it. The system should display the allowed range (e.g. 1–10 or 1–5) and perhaps the weight of each criterion. The judge’s scores for criteria are saved. The system calculates the judge’s **total weighted score** for the team automatically (sum of each score * criterion weight).
- **Score Validation:** The system ensures judges cannot submit incomplete scorecards. All required criteria must have scores. The score values must be within the defined range.
- **Minimum Comment Length:** A critical requirement from contest operations is that judges provide substantive written feedback. The platform enforces a minimum word count for the **overall comment** field. For example, in the Preliminary round require ≥ 60 words, Semi-Final ≥ 80 words, Final round – if doing live scoring – require a short justification ≥ 20 words per criterion score. These thresholds are configurable per stage via the rubric settings. The front-end can show a word counter to the judge and prevent submission until the requirement is met.
- **Submit & Edit Scores:** Judges can save their scoring form and come back to edit anytime before the scoring deadline. Once they finalize and submit, the assignment status becomes SUBMITTED. If needed (MVP simplification), we might allow judges to edit even after submission up until admin locks the stage for aggregation, to accommodate any changes of mind.
- **Outlier Detection:** As part of result calculation, the system will implement the outlier removal rule. Specifically, when computing final scores for a team, if the number of judges scoring that team is ≥ 5, drop the highest and lowest total score before averaging. (This rule is based on contest policy to mitigate bias extremes.) If fewer than 5 judges, or for simplicity in MVP, this rule can be configurable (e.g. always drop highest/lowest if 5 or more judges). The system should log which scores were dropped in the audit trail or result details for transparency.
### 5.7 Results & Awards

- **Stage Results:** After aggregation, the platform determines which teams advance or their ranking. For non-final stages, results mainly indicate **Advance** (yes/no) for each team (and possibly their rank or score). For the final stage, results include the prize or award for each team (e.g. First Place, Second Place, etc., or finalist). The system allows the admin to input or confirm these outcomes if automatic selection is not straight-forward (for example, if there’s a fixed number of finalists, the top N by score would be marked advance).
- **Awards Configuration:** Admin can configure award categories for the final stage (e.g. Gold, Silver, Bronze, “Best Innovation”, “Honorable Mention” etc.). Each award can be assigned to one or multiple teams after final scoring. The system should store these awards with the team records and allow publishing them.
- **Publishing Controls:** Admin has the option to **Publish to Teams** (i.e. teams can see their own results when they log in, and their feedback) versus **Publish Publicly** (e.g. updating a public “Winners” page or an API endpoint for public results). MVP can handle team-visible results immediately, and perhaps generate a Markdown or HTML snippet of winners for the public site. The publish action captures a timestamp and by whom in the audit log.
- **Feedback Package:** For transparency and educational value, the platform compiles a feedback report for each team. This “Feedback Pack” includes the scores (maybe criterion-by-criterion breakdown) and the written comments from all judges on that team, labeled generically (e.g. “Judge 1”, “Judge 2” without revealing names). Teams can download this as a PDF or view it on their portal. Providing this feedback is a key part of the contest’s learning mission.
### 5.8 Audit & Safety

- **Audit Logging:** The system logs critical actions and changes. For example: user registrations, team creations, submission uploads, score submissions, application approvals, judge assignment changes, result publications, etc. Each log entry records who (user ID) did what action, to which entity, and timestamp. Where applicable, before-and-after values or details are stored (e.g. what scores were changed). Admins can view these logs in the Admin console for monitoring and in case of any disputes or investigations.
- **Data Security & Privacy:** Use secure protocols for all communication. Passwords stored hashed (bcrypt). Personal data of students (like email, names) is accessible only to admins and their own team/judges as needed. Judge identities are not revealed to participants (and vice versa) to maintain blind review integrity. Ensure that file uploads are scanned for viruses (MVP: at least mark as “PENDING scan” and allow manual process).
- **Immutability of Scores:** Once final results are published, lock the scores to prevent any tampering. If an admin absolutely needs to adjust a score or result after publication (due to an error), the system should require a special override with an audit log entry (noting the reason). This ensures any changes post-publication are tracked.
## 6. UX / Pages (by Role)

A detailed breakdown of all pages, forms, and fields is provided in the  document. It enumerates each UI screen for students, teachers, judges, and admins along with the fields and validation rules. This ensures the front-end design aligns with the requirements above. Key interfaces include: public home/FAQ, user dashboards, team management pages, submission upload forms, judge scoring form, and admin management screens.

## 7. API Endpoints (MVP)

For a full list of backend REST API endpoints, see the . The API spec covers endpoints for authentication, retrieving contest info, team operations (create/join/team info), submissions, file uploads, judge actions, and admin actions (creating editions, assigning judges, publishing results, etc.). All endpoints follow REST conventions and are secured by role as described in the Permissions Matrix.

## 8. Data Model

The underlying data schema is documented in the . It lists all database tables, their columns, and relationships. Key entities include Users, Contests, Editions, Teams, Members, Applications, Stages, Requirements, Submissions, Files, Assignments, Scores, etc. Understanding the data model helps in implementation and future extension of features like tracking multiple contests or supporting more complex team structures.

## 9. Permissions Matrix

Refer to the  for a detailed role-based access matrix. It specifies which role can perform or view which actions/resources (e.g. students can only modify their own team data; judges can only see their assigned projects; admins have global access). This matrix is crucial for implementing security both in the frontend (routing/hiding UI) and backend (authorization checks).

## 10. Release Plan

We plan to deliver this MVP in a tight timeline of three weeks. The  outlines the development phases and milestones for each week (Week 1: core setup and team formation, Week 2: submissions and judging, Week 3: results aggregation, polish and deployment). Each week’s goals and deliverables are listed, along with P0 user stories that will be implemented.

## 11. Architecture & Deployment

The high-level architecture and repository structure is described in the . The system will be built as a Java Spring Boot backend and a React (Vite) frontend. We recommend a monorepo structure with clear module directories for backend, frontend, infra, and docs. The deployment in development uses Docker Compose (MySQL database, MinIO for storage, Mailpit for emails). In production, the same services can be deployed in containers or on cloud services. The architecture doc covers coding conventions, use of Flyway for DB migrations, and how to structure the front-end for multiple user roles (possibly a unified app or separate apps for admin/judge).
